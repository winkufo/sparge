## LLaMA 3 è°ƒä¼˜æŒ‡å—

### ğŸ“‹ ä¸CogVideoXçš„å…³é”®å·®å¼‚

| ç»´åº¦ | CogVideoX | LLaMA 3 |
|------|-----------|---------|
| **æ¨¡å‹ç±»å‹** | Diffusionè§†é¢‘ç”Ÿæˆ | Causalè¯­è¨€æ¨¡å‹ |
| **Attentionç±»å‹** | éå› æœ (is_causal=False) | å› æœ (is_causal=True) |
| **æ¶æ„ç‰¹ç‚¹** | ä½¿ç”¨Attention Processor | ç›´æ¥æ›¿æ¢Attentionå±‚ |
| **æ•°æ®æ ¼å¼** | è§†é¢‘ç”Ÿæˆprompts | æ–‡æœ¬æ•°æ®é›† |
| **è°ƒä¼˜æ•°æ®** | 5ä¸ªè§†é¢‘prompt | 5-10ä¸ªæ–‡æœ¬æ ·æœ¬ |
| **åºåˆ—é•¿åº¦** | å¯å˜ï¼ˆè§†é¢‘å¸§æ•°ï¼‰ | é€šå¸¸2048 tokens |
| **ç‰¹æ®Šç»„ä»¶** | RoPEåœ¨å›¾åƒéƒ¨åˆ† | RoPEåœ¨å…¨åºåˆ— + GQA |

### ğŸš€ å¿«é€Ÿå¼€å§‹

#### 1. è°ƒä¼˜ï¼ˆé¦–æ¬¡ä½¿ç”¨ï¼‰

```bash
# åŸºç¡€è°ƒä¼˜
python evaluate/llama_tune_example.py \
    --tune \
    --model_name meta-llama/Llama-3.2-1B \
    --model_out_path evaluate/models_dict/llama3_1b_tuned.pt \
    --l1 0.06 \
    --pv_l1 0.07

# å¹¶è¡Œè°ƒä¼˜ï¼ˆæ¨èï¼Œå¿«å¾ˆå¤šï¼‰
python evaluate/llama_tune_example.py \
    --tune \
    --parallel_tune \
    --model_name meta-llama/Llama-3.2-1B \
    --model_out_path evaluate/models_dict/llama3_1b_tuned.pt

# åªè°ƒä¼˜éƒ¨åˆ†å±‚ï¼ˆä¾‹å¦‚ï¼šåªè°ƒä¼˜ååŠéƒ¨åˆ†å±‚ï¼‰
python evaluate/llama_tune_example.py \
    --tune \
    --parallel_tune \
    --layer_range 16,32 \
    --model_out_path evaluate/models_dict/llama3_1b_partial.pt
```

**è°ƒä¼˜æ—¶é—´**ï¼š
- é¡ºåºè°ƒä¼˜ï¼š20-30åˆ†é’Ÿ
- å¹¶è¡Œè°ƒä¼˜ï¼š5-10åˆ†é’Ÿï¼ˆå¤šGPUï¼‰

#### 2. æ¨ç†ï¼ˆä½¿ç”¨è°ƒä¼˜å¥½çš„å‚æ•°ï¼‰

```bash
# ç”Ÿæˆæ–‡æœ¬æµ‹è¯•
python evaluate/llama_tune_example.py \
    --generate \
    --model_out_path evaluate/models_dict/llama3_1b_tuned.pt

# å¯ç”¨torch.compileè¿›ä¸€æ­¥åŠ é€Ÿ
python evaluate/llama_tune_example.py \
    --generate \
    --compile \
    --model_out_path evaluate/models_dict/llama3_1b_tuned.pt
```

#### 3. åœ¨ä½ è‡ªå·±çš„ä»£ç ä¸­ä½¿ç”¨

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
from evaluate.modify_model.modify_llama import set_spas_sage_attn_llama
from spas_sage_attn.autotune import load_sparse_attention_state_dict
import torch

# åŠ è½½æ¨¡å‹
model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-3.2-1B",
    torch_dtype=torch.bfloat16,
    device_map="auto"
)

# åº”ç”¨ç¨€ç–attention
model = set_spas_sage_attn_llama(
    model,
    l1=0.06,
    pv_l1=0.07
)

# åŠ è½½è°ƒä¼˜å¥½çš„å‚æ•°
tuned_params = torch.load("llama3_1b_tuned.pt")
load_sparse_attention_state_dict(model, tuned_params)

# æ­£å¸¸ä½¿ç”¨
tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-3.2-1B")
inputs = tokenizer("Hello, my name is", return_tensors="pt").to("cuda")
outputs = model.generate(**inputs, max_new_tokens=50)
```

---

## ğŸ“Š æŠ€æœ¯ç»†èŠ‚

### 1. LLaMAç‰¹æœ‰çš„å¤„ç†

#### Grouped Query Attention (GQA)

LLaMA 3ä½¿ç”¨GQAæ¥å‡å°‘KV cacheï¼š

```python
# LLaMA-3.2-1Bé…ç½®ç¤ºä¾‹
num_heads = 32           # Query heads
num_kv_heads = 8         # Key/Value headsï¼ˆæ›´å°‘ï¼ï¼‰
num_kv_groups = 4        # 32 / 8 = 4

# æˆ‘ä»¬çš„å®ç°è‡ªåŠ¨å¤„ç†GQA
key_states = self._repeat_kv(key_states, self.num_kv_groups)
value_states = self._repeat_kv(value_states, self.num_kv_groups)
```

#### Rotary Position Embedding (RoPE)

LLaMAä½¿ç”¨RoPEè€Œéç»å¯¹ä½ç½®ç¼–ç ï¼š

```python
# åœ¨Qå’ŒKä¸Šåº”ç”¨æ—‹è½¬ä½ç½®ç¼–ç 
cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len)
query_states, key_states = apply_rotary_pos_emb(
    query_states, key_states, cos, sin, position_ids
)
```

#### Causal Mask

LLaMAæ˜¯è‡ªå›å½’æ¨¡å‹ï¼Œå¿…é¡»ä½¿ç”¨causal maskï¼š

```python
# å¼ºåˆ¶ä½¿ç”¨causal attention
attn_output = sparse_attn(
    query, key, value,
    is_causal=True  # â† å…³é”®ï¼
)
```

### 2. ä¸CogVideoXå®ç°çš„å·®å¼‚

#### CogVideoXçš„å®ç°

```python
# cogvideo: ä½¿ç”¨Attention Processoræ¨¡å¼
class SageAttnCogVideoXAttnProcessor:
    def __call__(self, attn, hidden_states, ...):
        # åœ¨processorä¸­å¤„ç†attention
        query = attn.to_q(hidden_states)
        # ...
        hidden_states = attn.inner_attention(q, k, v, is_causal=False)

# è®¾ç½®processor
block.attn1.set_processor(SageAttnCogVideoXAttnProcessor())
```

#### LLaMAçš„å®ç°

```python
# llama: ç›´æ¥æ›¿æ¢æ•´ä¸ªAttentionæ¨¡å—
class SparseLlamaAttention(nn.Module):
    def __init__(self, original_attn):
        # å¤åˆ¶æ‰€æœ‰projectionå±‚
        self.q_proj = original_attn.q_proj
        self.k_proj = original_attn.k_proj
        # ...
    
    def forward(self, hidden_states, ...):
        # å®Œæ•´çš„attentioné€»è¾‘
        # åŒ…æ‹¬RoPEã€GQAã€KV cacheç­‰

# ç›´æ¥æ›¿æ¢
layer.self_attn = SparseLlamaAttention(original_attn)
```

**åŸå› **ï¼š
- CogVideoXçš„Attentionå·²ç»æœ‰processoræœºåˆ¶ï¼Œå¯ä»¥æ’å…¥è‡ªå®šä¹‰é€»è¾‘
- LLaMAçš„Attentionæ˜¯å®Œæ•´å°è£…çš„ï¼Œéœ€è¦æ•´ä½“æ›¿æ¢

---

## ğŸ¯ è°ƒä¼˜ç­–ç•¥

### ç­–ç•¥1ï¼šå…¨å±‚è°ƒä¼˜ï¼ˆé»˜è®¤ï¼‰

```bash
python evaluate/llama_tune_example.py --tune
# è°ƒä¼˜æ‰€æœ‰32å±‚ï¼ˆLLaMA-3.2-1Bï¼‰
```

**é€‚ç”¨åœºæ™¯**ï¼š
- è¿½æ±‚æœ€ä½³æ€§èƒ½
- æœ‰è¶³å¤Ÿçš„è°ƒä¼˜æ—¶é—´
- å…¨é¢éƒ¨ç½²

**é¢„æœŸæ•ˆæœ**ï¼š
- å¹³å‡ç¨€ç–åº¦ï¼š40-50%
- ç²¾åº¦æŸå¤±ï¼š<2%

### ç­–ç•¥2ï¼šåˆ†å±‚è°ƒä¼˜

ä¸åŒå±‚å¯¹ç¨€ç–åŒ–çš„æ•æ„Ÿåº¦ä¸åŒï¼š

```bash
# åªè°ƒä¼˜ååŠéƒ¨åˆ†ï¼ˆ16-31å±‚ï¼‰
python evaluate/llama_tune_example.py --tune --layer_range 16,32

# åªè°ƒä¼˜ä¸­é—´å±‚ï¼ˆ8-24å±‚ï¼‰
python evaluate/llama_tune_example.py --tune --layer_range 8,24
```

**ç»éªŒè§„åˆ™**ï¼š
- å‰å‡ å±‚ï¼ˆ0-7ï¼‰ï¼šæå–åŸºç¡€ç‰¹å¾ï¼Œå»ºè®®ä¿æŒå¯†é›†æˆ–ä½ç¨€ç–åº¦
- ä¸­é—´å±‚ï¼ˆ8-23ï¼‰ï¼šå¯ä»¥ä¸­ç­‰ç¨€ç–ï¼ˆ40-50%ï¼‰
- åæœŸå±‚ï¼ˆ24+ï¼‰ï¼šå¯ä»¥é«˜åº¦ç¨€ç–ï¼ˆ50-60%ï¼‰

### ç­–ç•¥3ï¼šè°ƒæ•´ç²¾åº¦çº¦æŸ

```bash
# é«˜ç²¾åº¦ï¼ˆä½ç¨€ç–ï¼‰
python evaluate/llama_tune_example.py --tune --l1 0.04 --pv_l1 0.05

# å¹³è¡¡ï¼ˆæ¨èï¼‰
python evaluate/llama_tune_example.py --tune --l1 0.06 --pv_l1 0.07

# é«˜ç¨€ç–ï¼ˆå¯èƒ½å½±å“ç²¾åº¦ï¼‰
python evaluate/llama_tune_example.py --tune --l1 0.10 --pv_l1 0.12
```

---

## ğŸ“ˆ è¯„ä¼°è°ƒä¼˜æ•ˆæœ

### æ–¹æ³•1ï¼šä½¿ç”¨æˆ‘ä»¬æä¾›çš„æµ‹è¯•è„šæœ¬

```bash
# æµ‹è¯•è°ƒä¼˜å‰åçš„è¾“å‡ºå·®å¼‚
python tests/test_llama_output_accuracy.py \
    --model meta-llama/Llama-3.2-1B \
    --output before_tune.json

# è°ƒä¼˜
python evaluate/llama_tune_example.py --tune --model_out_path tuned.pt

# æµ‹è¯•è°ƒä¼˜å
python tests/test_llama_output_accuracy.py \
    --model meta-llama/Llama-3.2-1B \
    --model_out_path tuned.pt \
    --output after_tune.json

# å¯¹æ¯”ç»“æœ
python compare_results.py before_tune.json after_tune.json
```

### æ–¹æ³•2ï¼šæŸ¥çœ‹ç¨€ç–åº¦ç»Ÿè®¡

```python
import json

# è¯»å–ç»Ÿè®¡æ–‡ä»¶
with open('evaluate/models_dict/llama3_stats.json') as f:
    stats = json.load(f)

# åˆ†æ
for layer_stats in stats:
    print(f"Layer {layer_stats['layer_idx']}: "
          f"ç¨€ç–åº¦ {layer_stats['mean_sparsity']:.2%}")
```

### æ–¹æ³•3ï¼šå®é™…ä»»åŠ¡æµ‹è¯•

```python
# åœ¨ä½ çš„ä¸‹æ¸¸ä»»åŠ¡ä¸Šæµ‹è¯•
from your_task import evaluate_model

# å¯†é›†æ¨¡å‹
score_dense = evaluate_model(dense_model)

# ç¨€ç–æ¨¡å‹
score_sparse = evaluate_model(sparse_model)

# å¯¹æ¯”
print(f"ç²¾åº¦å·®å¼‚: {(score_sparse - score_dense):.2%}")
```

---

## ğŸ”§ å¸¸è§é—®é¢˜

### Q1: è°ƒä¼˜æ—¶æ˜¾å­˜ä¸è¶³

**A**: å‡å°‘æ ·æœ¬é•¿åº¦æˆ–ä½¿ç”¨gradient checkpointing

```bash
# å‡å°‘åºåˆ—é•¿åº¦
python evaluate/llama_tune_example.py --tune --max_length 1024

# æˆ–ä½¿ç”¨å°æ ·æœ¬æ•°
python evaluate/llama_tune_example.py --tune --num_tune_samples 3
```

### Q2: è°ƒä¼˜å¾ˆæ…¢

**A**: ä½¿ç”¨å¹¶è¡Œè°ƒä¼˜

```bash
# ç¡®ä¿å¯ç”¨å¹¶è¡Œæ¨¡å¼
python evaluate/llama_tune_example.py --tune --parallel_tune

# è¿™ä¼šåˆ©ç”¨æ‰€æœ‰å¯ç”¨GPUå¹¶è¡Œå¤„ç†ä¸åŒçš„head
```

### Q3: ä¸åŒå¤§å°çš„LLaMAæ¨¡å‹éœ€è¦é‡æ–°è°ƒä¼˜å—ï¼Ÿ

**A**: æ˜¯çš„ï¼

```bash
# LLaMA-3.2-1B
python evaluate/llama_tune_example.py --tune \
    --model_name meta-llama/Llama-3.2-1B

# LLaMA-3.2-3Bï¼ˆéœ€è¦é‡æ–°è°ƒä¼˜ï¼‰
python evaluate/llama_tune_example.py --tune \
    --model_name meta-llama/Llama-3.2-3B \
    --model_out_path llama3_3b_tuned.pt
```

### Q4: è°ƒä¼˜åçš„å‚æ•°èƒ½è·¨ä»»åŠ¡ä½¿ç”¨å—ï¼Ÿ

**A**: å¯ä»¥ï¼Œä½†å»ºè®®åœ¨ç›®æ ‡ä»»åŠ¡æ•°æ®ä¸Šé‡æ–°è°ƒä¼˜

```bash
# é€šç”¨è°ƒä¼˜ï¼ˆWikiTextï¼‰
python evaluate/llama_tune_example.py --tune --dataset wikitext

# ç‰¹å®šä»»åŠ¡è°ƒä¼˜ï¼ˆä»£ç ï¼‰
python evaluate/llama_tune_example.py --tune --dataset codeparrot/github-code
```

### Q5: å¦‚ä½•çŸ¥é“è°ƒä¼˜æ˜¯å¦æˆåŠŸï¼Ÿ

**A**: çœ‹è¿™å‡ ä¸ªæŒ‡æ ‡ï¼š

1. **ç¨€ç–åº¦** > 30%ï¼ˆè‡³å°‘æœ‰æ•ˆæœï¼‰
2. **Logits Cosineç›¸ä¼¼åº¦** > 0.95ï¼ˆç²¾åº¦ä¿æŒï¼‰
3. **å®é™…ç”Ÿæˆè´¨é‡**ï¼ˆæœ€ç»ˆæ£€éªŒï¼‰

```bash
# è°ƒä¼˜åç«‹å³æµ‹è¯•ç”Ÿæˆ
python evaluate/llama_tune_example.py --generate --model_out_path tuned.pt
```

---

## ğŸ“ æœ€ä½³å®è·µæ€»ç»“

### âœ… æ¨èåšæ³•

1. **é¦–æ¬¡ä½¿ç”¨**ï¼šç”¨é»˜è®¤å‚æ•°è°ƒä¼˜
   ```bash
   python evaluate/llama_tune_example.py --tune --parallel_tune
   ```

2. **ç”Ÿäº§éƒ¨ç½²**ï¼šåœ¨ä»£è¡¨æ€§æ•°æ®ä¸Šè°ƒä¼˜
   - ä½¿ç”¨ä¸å®é™…åº”ç”¨ç›¸ä¼¼çš„æ–‡æœ¬
   - 5-10ä¸ªæ ·æœ¬å³å¯

3. **å®šæœŸè¯„ä¼°**ï¼šæ¨¡å‹æ›´æ–°åé‡æ–°è°ƒä¼˜
   - Fine-tuningåé‡æ–°è°ƒä¼˜
   - æ¢æ•°æ®åŸŸé‡æ–°è°ƒä¼˜

4. **ä¿å­˜å¤šä¸ªç‰ˆæœ¬**ï¼šä¸åŒç²¾åº¦-é€Ÿåº¦é…ç½®
   ```bash
   # é«˜ç²¾åº¦ç‰ˆ
   python evaluate/llama_tune_example.py --tune --l1 0.04 \
       --model_out_path llama3_high_accuracy.pt
   
   # é«˜é€Ÿåº¦ç‰ˆ
   python evaluate/llama_tune_example.py --tune --l1 0.08 \
       --model_out_path llama3_high_speed.pt
   ```

### âŒ é¿å…çš„åšæ³•

1. âŒ æ¯æ¬¡æ¨ç†éƒ½è°ƒä¼˜ï¼ˆå¤ªæ…¢ï¼‰
2. âŒ ç”¨å¤ªçŸ­çš„æ–‡æœ¬è°ƒä¼˜ï¼ˆ<256 tokensï¼‰
3. âŒ è·¨æ¨¡å‹å…±ç”¨è°ƒä¼˜å‚æ•°
4. âŒ ä¸éªŒè¯å°±ç›´æ¥éƒ¨ç½²

---

## ğŸ“ è¿›é˜¶ï¼šè‡ªå®šä¹‰è°ƒä¼˜

å¦‚æœé»˜è®¤è„šæœ¬ä¸æ»¡è¶³éœ€æ±‚ï¼Œå¯ä»¥è‡ªå®šä¹‰ï¼š

```python
from transformers import AutoModelForCausalLM
from evaluate.modify_model.modify_llama import set_spas_sage_attn_llama, enable_tune_mode
from spas_sage_attn.autotune import extract_sparse_attention_state_dict

# 1. åŠ è½½æ¨¡å‹
model = AutoModelForCausalLM.from_pretrained("your_model")

# 2. è‡ªå®šä¹‰å±‚èŒƒå›´å’Œå‚æ•°
model = set_spas_sage_attn_llama(
    model,
    l1=0.05,           # è‡ªå®šä¹‰ç²¾åº¦
    pv_l1=0.06,
    layer_range=(10, 30)  # åªæ›¿æ¢10-29å±‚
)

# 3. å¯ç”¨è°ƒä¼˜
enable_tune_mode(model, True)

# 4. ç”¨ä½ è‡ªå·±çš„æ•°æ®è°ƒä¼˜
for text in your_custom_data:
    inputs = tokenizer(text, return_tensors="pt")
    model(**inputs)  # è§¦å‘è°ƒä¼˜

# 5. ä¿å­˜
params = extract_sparse_attention_state_dict(model)
torch.save(params, "custom_tuned.pt")
```

---

## ğŸ“š ç›¸å…³æ–‡æ¡£

- [æµ‹è¯•æŒ‡å—](../tests/README.md)
- [è°ƒä¼˜åŸç†](../docs/tuning_guide.md)
- [CogVideoXè°ƒä¼˜](cogvideo_example.py)

