"""
CogVideoXæ¨¡å‹å‡†ç¡®æ€§æµ‹è¯•è„šæœ¬

å¯¹æ¯”ä½¿ç”¨SpargeAttn vs ä¸ä½¿ç”¨SpargeAttnçš„è¾“å‡ºå·®å¼‚

ç”¨æ³•:
    # æµ‹è¯•ä¸è°ƒä¼˜çš„ç¨€ç–attention
    python evaluate/cogvideo_accuracy_test.py
    
    # æµ‹è¯•è°ƒä¼˜åçš„ç¨€ç–attention
    python evaluate/cogvideo_accuracy_test.py \
        --use_tuned \
        --tuned_path evaluate/models_dict/CogVideoX-2b_0.06_0.07.pt
    
    # æŒ‡å®šæµ‹è¯•prompts
    python evaluate/cogvideo_accuracy_test.py \
        --prompt_file evaluate/datasets/video/test_prompts.txt
"""

import torch
import os
import gc
import argparse
import json
import numpy as np
from tqdm import tqdm
from diffusers import CogVideoXPipeline
from diffusers.models import CogVideoXTransformer3DModel
from diffusers.utils import export_to_video

from modify_model.modify_cogvideo import set_spas_sage_attn_cogvideox
from spas_sage_attn.autotune import (
    extract_sparse_attention_state_dict,
    load_sparse_attention_state_dict,
)
from spas_sage_attn.utils import precision_metric


def parse_args():
    parser = argparse.ArgumentParser(description="CogVideoX Accuracy Test")
    
    # æ¨¡å‹é…ç½®
    parser.add_argument(
        "--model_name",
        type=str,
        default="THUDM/CogVideoX-2b",
        help="æ¨¡å‹åç§°"
    )
    
    # ç¨€ç–é…ç½®
    parser.add_argument(
        "--use_tuned",
        action="store_true",
        help="ä½¿ç”¨è°ƒä¼˜åçš„å‚æ•°"
    )
    parser.add_argument(
        "--tuned_path",
        type=str,
        default="evaluate/models_dict/CogVideoX-2b_0.06_0.07.pt",
        help="è°ƒä¼˜å‚æ•°è·¯å¾„"
    )
    parser.add_argument("--l1", type=float, default=0.06, help="L1è¯¯å·®ä¸Šé™")
    parser.add_argument("--pv_l1", type=float, default=0.065, help="PV L1è¯¯å·®ä¸Šé™")
    parser.add_argument(
        "--simthreshd1",
        type=float,
        default=0.6,
        help="ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆä¸è°ƒä¼˜æ—¶ä½¿ç”¨ï¼‰"
    )
    parser.add_argument(
        "--cdfthreshd",
        type=float,
        default=0.98,
        help="CDFé˜ˆå€¼ï¼ˆä¸è°ƒä¼˜æ—¶ä½¿ç”¨ï¼‰"
    )
    
    # æµ‹è¯•é…ç½®
    parser.add_argument(
        "--prompt_file",
        type=str,
        default="evaluate/datasets/video/prompts.txt",
        help="æµ‹è¯•promptsæ–‡ä»¶"
    )
    parser.add_argument(
        "--num_prompts",
        type=int,
        default=5,
        help="æµ‹è¯•promptæ•°é‡"
    )
    parser.add_argument(
        "--num_inference_steps",
        type=int,
        default=50,
        help="æ¨ç†æ­¥æ•°"
    )
    parser.add_argument(
        "--num_frames",
        type=int,
        default=49,
        help="ç”Ÿæˆå¸§æ•°"
    )
    parser.add_argument(
        "--guidance_scale",
        type=float,
        default=6.0,
        help="Guidance scale"
    )
    parser.add_argument(
        "--seed",
        type=int,
        default=42,
        help="éšæœºç§å­"
    )
    
    # è¾“å‡ºé…ç½®
    parser.add_argument(
        "--output_dir",
        type=str,
        default="evaluate/results/cogvideo_accuracy",
        help="ç»“æœè¾“å‡ºç›®å½•"
    )
    parser.add_argument(
        "--save_videos",
        action="store_true",
        help="ä¿å­˜ç”Ÿæˆçš„è§†é¢‘"
    )
    parser.add_argument("--verbose", action="store_true", help="è¯¦ç»†è¾“å‡º")
    
    # æµ‹è¯•é€‰é¡¹
    parser.add_argument(
        "--test_intermediate",
        action="store_true",
        help="æµ‹è¯•ä¸­é—´å±‚è¾“å‡ºï¼ˆæ›´è¯¦ç»†ä½†æ›´æ…¢ï¼‰"
    )
    
    args = parser.parse_args()
    return args


def load_prompts(prompt_file, num_prompts=5):
    """
    åŠ è½½æµ‹è¯•prompts
    """
    print(f"\nåŠ è½½æµ‹è¯•prompts: {prompt_file}")
    
    if not os.path.exists(prompt_file):
        # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤prompts
        print("âš ï¸  Promptæ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤prompts")
        prompts = [
            "A panda eating bamboo in a lush forest.",
            "A car driving on a mountain road during sunset.",
            "A cat playing with a ball of yarn.",
            "Ocean waves crashing on a beach.",
            "A city street with people walking."
        ]
    else:
        with open(prompt_file, 'r', encoding='utf-8') as f:
            prompts = [line.strip() for line in f.readlines() if line.strip()]
    
    prompts = prompts[:num_prompts]
    print(f"âœ“ åŠ è½½äº† {len(prompts)} ä¸ªæµ‹è¯•prompts")
    for i, p in enumerate(prompts):
        print(f"  {i+1}. {p}")
    
    return prompts


def create_pipelines(args):
    """
    åˆ›å»ºä¸¤ä¸ªpipelineï¼šå¯†é›†ç‰ˆå’Œç¨€ç–ç‰ˆ
    """
    print("\n" + "="*60)
    print("åˆ›å»ºæµ‹è¯•Pipeline")
    print("="*60)
    
    device = "cuda" if torch.cuda.is_available() else "cpu"
    dtype = torch.bfloat16
    
    # 1. åˆ›å»ºå¯†é›†ç‰ˆæœ¬ï¼ˆbaselineï¼‰
    print(f"\n[1/2] åˆ›å»ºå¯†é›†ç‰ˆPipeline (baseline)")
    
    transformer_dense = CogVideoXTransformer3DModel.from_pretrained(
        args.model_name,
        subfolder="transformer",
        torch_dtype=dtype,
    )
    
    pipe_dense = CogVideoXPipeline.from_pretrained(
        args.model_name,
        transformer=transformer_dense,
        torch_dtype=dtype,
    ).to(device)
    
    pipe_dense.enable_model_cpu_offload()
    print("âœ“ å¯†é›†ç‰ˆPipelineåˆ›å»ºå®Œæˆ")
    
    # 2. åˆ›å»ºç¨€ç–ç‰ˆæœ¬
    print(f"\n[2/2] åˆ›å»ºç¨€ç–ç‰ˆPipeline")
    
    transformer_sparse = CogVideoXTransformer3DModel.from_pretrained(
        args.model_name,
        subfolder="transformer",
        torch_dtype=dtype,
    )
    
    # åº”ç”¨ç¨€ç–attention
    set_spas_sage_attn_cogvideox(
        transformer_sparse,
        verbose=args.verbose,
        l1=args.l1,
        pv_l1=args.pv_l1
    )
    
    # åŠ è½½è°ƒä¼˜å‚æ•°ï¼ˆå¦‚æœæŒ‡å®šï¼‰
    if args.use_tuned:
        if os.path.exists(args.tuned_path):
            print(f"âœ“ åŠ è½½è°ƒä¼˜å‚æ•°: {args.tuned_path}")
            tuned_params = torch.load(args.tuned_path)
            load_sparse_attention_state_dict(
                transformer_sparse,
                tuned_params,
                verbose=args.verbose
            )
        else:
            print(f"âš ï¸  æœªæ‰¾åˆ°è°ƒä¼˜å‚æ•°: {args.tuned_path}")
            print("å°†ä½¿ç”¨é»˜è®¤å‚æ•°")
    else:
        print(f"ä½¿ç”¨é»˜è®¤å‚æ•°: simthreshd1={args.simthreshd1}, cdfthreshd={args.cdfthreshd}")
    
    pipe_sparse = CogVideoXPipeline.from_pretrained(
        args.model_name,
        transformer=transformer_sparse,
        torch_dtype=dtype,
    ).to(device)
    
    pipe_sparse.enable_model_cpu_offload()
    print("âœ“ ç¨€ç–ç‰ˆPipelineåˆ›å»ºå®Œæˆ")
    
    return pipe_dense, pipe_sparse


def test_single_prompt(
    pipe_dense,
    pipe_sparse,
    prompt,
    args,
    save_prefix="test"
):
    """
    æµ‹è¯•å•ä¸ªpromptçš„è¾“å‡ºå·®å¼‚
    """
    generator_dense = torch.Generator(device="cuda").manual_seed(args.seed)
    generator_sparse = torch.Generator(device="cuda").manual_seed(args.seed)
    
    # 1. å¯†é›†ç‰ˆç”Ÿæˆ
    print(f"\n  ç”Ÿæˆå¯†é›†ç‰ˆè§†é¢‘...")
    with torch.no_grad():
        # å¦‚æœéœ€è¦æµ‹è¯•ä¸­é—´å±‚ï¼Œè®¾ç½®output_hidden_states
        if args.test_intermediate:
            # æ³¨æ„ï¼šè¿™éœ€è¦ä¿®æ”¹pipelineæ¥æ”¯æŒè¿”å›ä¸­é—´çŠ¶æ€
            outputs_dense = pipe_dense(
                prompt.strip(),
                num_videos_per_prompt=1,
                num_inference_steps=args.num_inference_steps,
                num_frames=args.num_frames,
                guidance_scale=args.guidance_scale,
                generator=generator_dense,
            )
        else:
            outputs_dense = pipe_dense(
                prompt.strip(),
                num_videos_per_prompt=1,
                num_inference_steps=args.num_inference_steps,
                num_frames=args.num_frames,
                guidance_scale=args.guidance_scale,
                generator=generator_dense,
            )
    
    frames_dense = outputs_dense.frames[0]
    
    # 2. ç¨€ç–ç‰ˆç”Ÿæˆ
    print(f"  ç”Ÿæˆç¨€ç–ç‰ˆè§†é¢‘...")
    with torch.no_grad():
        outputs_sparse = pipe_sparse(
            prompt.strip(),
            num_videos_per_prompt=1,
            num_inference_steps=args.num_inference_steps,
            num_frames=args.num_frames,
            guidance_scale=args.guidance_scale,
            generator=generator_sparse,
        )
    
    frames_sparse = outputs_sparse.frames[0]
    
    # 3. å¯¹æ¯”è§†é¢‘å¸§
    print(f"  å¯¹æ¯”ç”Ÿæˆçš„è§†é¢‘...")
    
    # è½¬æ¢ä¸ºtensorè¿›è¡Œå¯¹æ¯”
    frames_dense_tensor = torch.from_numpy(
        np.array(frames_dense)
    ).float() / 255.0  # [T, H, W, C]
    
    frames_sparse_tensor = torch.from_numpy(
        np.array(frames_sparse)
    ).float() / 255.0
    
    # è®¡ç®—å¸§çº§åˆ«çš„ç›¸ä¼¼åº¦
    frame_metrics = precision_metric(
        frames_sparse_tensor,
        frames_dense_tensor,
        verbose=False
    )
    
    # è®¡ç®—æ¯å¸§çš„å·®å¼‚
    per_frame_diffs = []
    for t in range(len(frames_dense)):
        frame_d = frames_dense_tensor[t]
        frame_s = frames_sparse_tensor[t]
        diff = (frame_d - frame_s).abs().mean().item()
        per_frame_diffs.append(diff)
    
    # è®¡ç®—PSNRå’ŒSSIMï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
    mse = ((frames_dense_tensor - frames_sparse_tensor) ** 2).mean().item()
    psnr = 10 * np.log10(1.0 / (mse + 1e-10))
    
    result = {
        'prompt': prompt,
        'num_frames': len(frames_dense),
        'frame_cosine': frame_metrics['Cossim'],
        'frame_l1': frame_metrics['L1'],
        'frame_rmse': frame_metrics['RMSE'],
        'psnr': psnr,
        'per_frame_diff_mean': np.mean(per_frame_diffs),
        'per_frame_diff_max': np.max(per_frame_diffs),
        'per_frame_diffs': per_frame_diffs,
    }
    
    # 4. ä¿å­˜è§†é¢‘ï¼ˆå¦‚æœéœ€è¦ï¼‰
    if args.save_videos:
        video_dir = os.path.join(args.output_dir, "videos")
        os.makedirs(video_dir, exist_ok=True)
        
        dense_path = os.path.join(video_dir, f"{save_prefix}_dense.mp4")
        sparse_path = os.path.join(video_dir, f"{save_prefix}_sparse.mp4")
        
        export_to_video(frames_dense, dense_path, fps=8)
        export_to_video(frames_sparse, sparse_path, fps=8)
        
        result['dense_video_path'] = dense_path
        result['sparse_video_path'] = sparse_path
        
        print(f"  âœ“ è§†é¢‘å·²ä¿å­˜: {dense_path}, {sparse_path}")
    
    return result


def test_all_prompts(pipe_dense, pipe_sparse, prompts, args):
    """
    æµ‹è¯•æ‰€æœ‰prompts
    """
    print("\n" + "="*60)
    print("å¼€å§‹æµ‹è¯•")
    print("="*60)
    
    all_results = []
    
    for idx, prompt in enumerate(prompts):
        print(f"\n{'='*60}")
        print(f"æµ‹è¯• {idx+1}/{len(prompts)}")
        print(f"Prompt: {prompt}")
        print(f"{'='*60}")
        
        try:
            result = test_single_prompt(
                pipe_dense,
                pipe_sparse,
                prompt,
                args,
                save_prefix=f"prompt_{idx}"
            )
            all_results.append(result)
            
            # æ‰“å°å½“å‰ç»“æœ
            print(f"\n  ç»“æœ:")
            print(f"    å¸§ç›¸ä¼¼åº¦(Cosine): {result['frame_cosine']:.6f}")
            print(f"    å¸§L1è¯¯å·®: {result['frame_l1']:.6f}")
            print(f"    PSNR: {result['psnr']:.2f} dB")
            print(f"    å¹³å‡å¸§å·®å¼‚: {result['per_frame_diff_mean']:.6f}")
            
        except Exception as e:
            print(f"  âœ— æµ‹è¯•å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
        
        # æ¸…ç†æ˜¾å­˜
        gc.collect()
        torch.cuda.empty_cache()
    
    return all_results


def compute_statistics(all_results):
    """
    è®¡ç®—ç»Ÿè®¡æ‘˜è¦
    """
    print("\n" + "="*60)
    print("ç»Ÿè®¡æ‘˜è¦")
    print("="*60)
    
    if not all_results:
        print("æ²¡æœ‰æœ‰æ•ˆçš„æµ‹è¯•ç»“æœ")
        return {}
    
    stats = {}
    
    metrics = ['frame_cosine', 'frame_l1', 'frame_rmse', 'psnr', 
               'per_frame_diff_mean', 'per_frame_diff_max']
    
    for metric in metrics:
        values = [r[metric] for r in all_results]
        stats[metric] = {
            'mean': np.mean(values),
            'std': np.std(values),
            'min': np.min(values),
            'max': np.max(values),
        }
    
    # æ‰“å°è¡¨æ ¼
    print(f"\n{'æŒ‡æ ‡':<25} {'å‡å€¼':<12} {'æ ‡å‡†å·®':<12} {'æœ€å°å€¼':<12} {'æœ€å¤§å€¼':<12}")
    print("-" * 73)
    
    metrics_display = [
        ('frame_cosine', 'å¸§ç›¸ä¼¼åº¦(Cosine)'),
        ('frame_l1', 'å¸§L1è¯¯å·®'),
        ('psnr', 'PSNR (dB)'),
        ('per_frame_diff_mean', 'å¹³å‡å¸§å·®å¼‚'),
    ]
    
    for key, display_name in metrics_display:
        s = stats[key]
        print(f"{display_name:<25} {s['mean']:<12.6f} {s['std']:<12.6f} "
              f"{s['min']:<12.6f} {s['max']:<12.6f}")
    
    return stats


def evaluate_quality(stats):
    """
    è¯„ä¼°æ•´ä½“è´¨é‡
    """
    print("\n" + "="*60)
    print("è´¨é‡è¯„ä¼°")
    print("="*60)
    
    if not stats:
        print("æ— æ³•è¯„ä¼°ï¼ˆæ²¡æœ‰ç»Ÿè®¡æ•°æ®ï¼‰")
        return 0
    
    score = 0
    max_score = 100
    
    # å¸§ç›¸ä¼¼åº¦ (40åˆ†)
    frame_cosine = stats['frame_cosine']['mean']
    if frame_cosine > 0.98:
        score += 40
        print(f"âœ“ å¸§ç›¸ä¼¼åº¦: {frame_cosine:.4f} (ä¼˜ç§€) - 40/40åˆ†")
    elif frame_cosine > 0.95:
        score += 30
        print(f"âœ“ å¸§ç›¸ä¼¼åº¦: {frame_cosine:.4f} (è‰¯å¥½) - 30/40åˆ†")
    elif frame_cosine > 0.90:
        score += 20
        print(f"âš  å¸§ç›¸ä¼¼åº¦: {frame_cosine:.4f} (å¯æ¥å—) - 20/40åˆ†")
    else:
        score += 10
        print(f"âœ— å¸§ç›¸ä¼¼åº¦: {frame_cosine:.4f} (éœ€æ”¹è¿›) - 10/40åˆ†")
    
    # PSNR (40åˆ†) - è¶Šé«˜è¶Šå¥½
    psnr = stats['psnr']['mean']
    if psnr > 40:
        score += 40
        print(f"âœ“ PSNR: {psnr:.2f} dB (ä¼˜ç§€) - 40/40åˆ†")
    elif psnr > 35:
        score += 30
        print(f"âœ“ PSNR: {psnr:.2f} dB (è‰¯å¥½) - 30/40åˆ†")
    elif psnr > 30:
        score += 20
        print(f"âš  PSNR: {psnr:.2f} dB (å¯æ¥å—) - 20/40åˆ†")
    else:
        score += 10
        print(f"âœ— PSNR: {psnr:.2f} dB (éœ€æ”¹è¿›) - 10/40åˆ†")
    
    # å¸§L1è¯¯å·® (20åˆ†) - è¶Šå°è¶Šå¥½
    frame_l1 = stats['frame_l1']['mean']
    if frame_l1 < 0.02:
        score += 20
        print(f"âœ“ å¸§L1è¯¯å·®: {frame_l1:.6f} (ä¼˜ç§€) - 20/20åˆ†")
    elif frame_l1 < 0.05:
        score += 15
        print(f"âœ“ å¸§L1è¯¯å·®: {frame_l1:.6f} (è‰¯å¥½) - 15/20åˆ†")
    elif frame_l1 < 0.10:
        score += 10
        print(f"âš  å¸§L1è¯¯å·®: {frame_l1:.6f} (å¯æ¥å—) - 10/20åˆ†")
    else:
        score += 5
        print(f"âœ— å¸§L1è¯¯å·®: {frame_l1:.6f} (éœ€æ”¹è¿›) - 5/20åˆ†")
    
    # æ€»è¯„
    print(f"\n{'='*60}")
    print(f"æ€»åˆ†: {score}/{max_score}")
    print(f"{'='*60}")
    
    if score >= 90:
        print("ğŸ‰ ä¼˜ç§€ï¼è§†é¢‘è´¨é‡å‡ ä¹æ— æŸ")
    elif score >= 75:
        print("âœ“ è‰¯å¥½ï¼Œè§†é¢‘è´¨é‡ä¿æŒå¾—å¾ˆå¥½")
    elif score >= 60:
        print("âš  å¯æ¥å—ï¼Œæœ‰è½»å¾®è´¨é‡ä¸‹é™")
    else:
        print("âœ— éœ€è¦æ”¹è¿›ï¼Œå»ºè®®è°ƒæ•´å‚æ•°æˆ–é‡æ–°è°ƒä¼˜")
    
    return score


def save_results(args, all_results, stats, score):
    """
    ä¿å­˜ç»“æœåˆ°æ–‡ä»¶
    """
    os.makedirs(args.output_dir, exist_ok=True)
    
    # ç”Ÿæˆæ–‡ä»¶å
    if args.use_tuned:
        filename = "cogvideo_accuracy_tuned.json"
    else:
        filename = "cogvideo_accuracy_default.json"
    
    output_path = os.path.join(args.output_dir, filename)
    
    # æ±‡æ€»ç»“æœ
    results = {
        'config': {
            'model_name': args.model_name,
            'use_tuned': args.use_tuned,
            'tuned_path': args.tuned_path if args.use_tuned else None,
            'l1': args.l1,
            'pv_l1': args.pv_l1,
            'num_prompts': len(all_results),
            'num_inference_steps': args.num_inference_steps,
            'num_frames': args.num_frames,
            'seed': args.seed,
        },
        'statistics': stats,
        'detailed_results': all_results,
        'overall_score': score
    }
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    print(f"\nâœ“ ç»“æœå·²ä¿å­˜è‡³: {output_path}")


def main():
    args = parse_args()
    
    print("\n" + "="*80)
    print("CogVideoXæ¨¡å‹å‡†ç¡®æ€§æµ‹è¯•")
    print("="*80)
    print(f"æ¨¡å‹: {args.model_name}")
    print(f"ä½¿ç”¨è°ƒä¼˜å‚æ•°: {args.use_tuned}")
    if args.use_tuned:
        print(f"è°ƒä¼˜å‚æ•°è·¯å¾„: {args.tuned_path}")
    print(f"æµ‹è¯•promptsæ•°: {args.num_prompts}")
    print(f"æ¨ç†æ­¥æ•°: {args.num_inference_steps}")
    print(f"ç”Ÿæˆå¸§æ•°: {args.num_frames}")
    
    # 1. åŠ è½½prompts
    prompts = load_prompts(args.prompt_file, args.num_prompts)
    
    # 2. åˆ›å»ºpipelines
    pipe_dense, pipe_sparse = create_pipelines(args)
    
    # 3. æµ‹è¯•æ‰€æœ‰prompts
    all_results = test_all_prompts(pipe_dense, pipe_sparse, prompts, args)
    
    # 4. è®¡ç®—ç»Ÿè®¡
    stats = compute_statistics(all_results)
    
    # 5. è¯„ä¼°è´¨é‡
    score = evaluate_quality(stats)
    
    # 6. ä¿å­˜ç»“æœ
    save_results(args, all_results, stats, score)
    
    print("\n" + "="*80)
    print("æµ‹è¯•å®Œæˆï¼")
    print("="*80)
    
    if args.save_videos:
        print(f"\nç”Ÿæˆçš„è§†é¢‘ä¿å­˜åœ¨: {os.path.join(args.output_dir, 'videos')}")


if __name__ == "__main__":
    main()

