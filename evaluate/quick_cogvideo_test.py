"""
CogVideoXå¿«é€Ÿå‡†ç¡®æ€§æµ‹è¯•

å¿«é€ŸéªŒè¯ç¨€ç–åŒ–æ•ˆæœï¼Œåªç”Ÿæˆ1ä¸ªè§†é¢‘è¿›è¡Œå¯¹æ¯”
è¿è¡Œæ—¶é—´: ~5åˆ†é’Ÿ

ç”¨æ³•:
    python evaluate/quick_cogvideo_test.py
    python evaluate/quick_cogvideo_test.py --use_tuned --tuned_path your_tuned.pt
"""

import torch
import os
import gc
import argparse
import numpy as np
from diffusers import CogVideoXPipeline
from diffusers.models import CogVideoXTransformer3DModel
from diffusers.utils import export_to_video

from modify_model.modify_cogvideo import set_spas_sage_attn_cogvideox
from spas_sage_attn.autotune import load_sparse_attention_state_dict
from spas_sage_attn.utils import precision_metric


def parse_args():
    parser = argparse.ArgumentParser(description="CogVideoX Quick Test")
    parser.add_argument("--use_tuned", action="store_true", help="ä½¿ç”¨è°ƒä¼˜å‚æ•°")
    parser.add_argument(
        "--tuned_path",
        type=str,
        default="evaluate/models_dict/CogVideoX-2b_0.06_0.07.pt",
        help="è°ƒä¼˜å‚æ•°è·¯å¾„"
    )
    parser.add_argument("--l1", type=float, default=0.06, help="L1è¯¯å·®ä¸Šé™")
    parser.add_argument("--pv_l1", type=float, default=0.065, help="PV L1è¯¯å·®ä¸Šé™")
    parser.add_argument(
        "--prompt",
        type=str,
        default="A panda eating bamboo in a lush forest.",
        help="æµ‹è¯•prompt"
    )
    parser.add_argument("--save_videos", action="store_true", help="ä¿å­˜è§†é¢‘")
    
    return parser.parse_args()


def main():
    args = parse_args()
    
    print("\n" + "="*60)
    print("CogVideoXå¿«é€Ÿå‡†ç¡®æ€§æµ‹è¯•")
    print("="*60)
    print(f"Prompt: {args.prompt}")
    print(f"ä½¿ç”¨è°ƒä¼˜å‚æ•°: {args.use_tuned}")
    
    device = "cuda" if torch.cuda.is_available() else "cpu"
    dtype = torch.bfloat16
    num_frames = 49
    seed = 42
    
    # ===== 1. å¯†é›†ç‰ˆï¼ˆbaselineï¼‰=====
    print("\n[1/4] åˆ›å»ºå¯†é›†ç‰ˆPipeline...")
    
    transformer_dense = CogVideoXTransformer3DModel.from_pretrained(
        "THUDM/CogVideoX-2b",
        subfolder="transformer",
        torch_dtype=dtype,
    )
    
    pipe_dense = CogVideoXPipeline.from_pretrained(
        "THUDM/CogVideoX-2b",
        transformer=transformer_dense,
        torch_dtype=dtype,
    ).to(device)
    
    pipe_dense.enable_model_cpu_offload()
    
    print("ç”Ÿæˆå¯†é›†ç‰ˆè§†é¢‘...")
    with torch.no_grad():
        video_dense = pipe_dense(
            args.prompt,
            num_videos_per_prompt=1,
            num_inference_steps=50,
            num_frames=num_frames,
            guidance_scale=6,
            generator=torch.Generator(device="cuda").manual_seed(seed),
        ).frames[0]
    
    print(f"âœ“ å¯†é›†ç‰ˆç”Ÿæˆå®Œæˆ ({len(video_dense)}å¸§)")
    
    # æ¸…ç†
    del pipe_dense, transformer_dense
    gc.collect()
    torch.cuda.empty_cache()
    
    # ===== 2. ç¨€ç–ç‰ˆ =====
    print("\n[2/4] åˆ›å»ºç¨€ç–ç‰ˆPipeline...")
    
    transformer_sparse = CogVideoXTransformer3DModel.from_pretrained(
        "THUDM/CogVideoX-2b",
        subfolder="transformer",
        torch_dtype=dtype,
    )
    
    # åº”ç”¨ç¨€ç–attention
    set_spas_sage_attn_cogvideox(
        transformer_sparse,
        verbose=False,
        l1=args.l1,
        pv_l1=args.pv_l1
    )
    
    # åŠ è½½è°ƒä¼˜å‚æ•°
    if args.use_tuned and os.path.exists(args.tuned_path):
        print(f"âœ“ åŠ è½½è°ƒä¼˜å‚æ•°: {args.tuned_path}")
        tuned_params = torch.load(args.tuned_path)
        load_sparse_attention_state_dict(transformer_sparse, tuned_params)
    
    pipe_sparse = CogVideoXPipeline.from_pretrained(
        "THUDM/CogVideoX-2b",
        transformer=transformer_sparse,
        torch_dtype=dtype,
    ).to(device)
    
    pipe_sparse.enable_model_cpu_offload()
    
    print("ç”Ÿæˆç¨€ç–ç‰ˆè§†é¢‘...")
    with torch.no_grad():
        video_sparse = pipe_sparse(
            args.prompt,
            num_videos_per_prompt=1,
            num_inference_steps=50,
            num_frames=num_frames,
            guidance_scale=6,
            generator=torch.Generator(device="cuda").manual_seed(seed),
        ).frames[0]
    
    print(f"âœ“ ç¨€ç–ç‰ˆç”Ÿæˆå®Œæˆ ({len(video_sparse)}å¸§)")
    
    # ===== 3. å¯¹æ¯”å·®å¼‚ =====
    print("\n[3/4] å¯¹æ¯”è§†é¢‘å·®å¼‚...")
    
    # è½¬æ¢ä¸ºtensor
    frames_dense = torch.from_numpy(np.array(video_dense)).float() / 255.0
    frames_sparse = torch.from_numpy(np.array(video_sparse)).float() / 255.0
    
    print(f"è§†é¢‘å½¢çŠ¶: {frames_dense.shape}")  # [T, H, W, C]
    
    # è®¡ç®—æ•´ä½“ç›¸ä¼¼åº¦
    metrics = precision_metric(frames_sparse, frames_dense, verbose=False)
    
    # è®¡ç®—PSNR
    mse = ((frames_dense - frames_sparse) ** 2).mean().item()
    psnr = 10 * np.log10(1.0 / (mse + 1e-10))
    
    # è®¡ç®—æ¯å¸§å·®å¼‚
    per_frame_diffs = []
    for t in range(len(video_dense)):
        diff = (frames_dense[t] - frames_sparse[t]).abs().mean().item()
        per_frame_diffs.append(diff)
    
    # ===== 4. ç»“æœ =====
    print("\n[4/4] æµ‹è¯•ç»“æœ")
    print("="*60)
    print(f"å¸§ç›¸ä¼¼åº¦(Cosine): {metrics['Cossim']:.6f}")
    print(f"å¸§L1è¯¯å·®: {metrics['L1']:.6f}")
    print(f"å¸§RMSE: {metrics['RMSE']:.6f}")
    print(f"PSNR: {psnr:.2f} dB")
    print(f"å¹³å‡å¸§å·®å¼‚: {np.mean(per_frame_diffs):.6f}")
    print(f"æœ€å¤§å¸§å·®å¼‚: {np.max(per_frame_diffs):.6f}")
    
    # è¯„ä¼°
    print("\n" + "="*60)
    print("è´¨é‡è¯„ä¼°")
    print("="*60)
    
    if metrics['Cossim'] > 0.98 and psnr > 40:
        print("ğŸ‰ ä¼˜ç§€ï¼è§†é¢‘è´¨é‡å‡ ä¹æ— æŸ")
        quality = "excellent"
    elif metrics['Cossim'] > 0.95 and psnr > 35:
        print("âœ“ è‰¯å¥½ï¼è§†é¢‘è´¨é‡ä¿æŒå¾—å¾ˆå¥½")
        quality = "good"
    elif metrics['Cossim'] > 0.90 and psnr > 30:
        print("âš  å¯æ¥å—ï¼Œæœ‰è½»å¾®è´¨é‡ä¸‹é™")
        quality = "acceptable"
    else:
        print("âœ— éœ€è¦æ”¹è¿›ï¼Œå»ºè®®è°ƒæ•´å‚æ•°")
        quality = "needs_improvement"
    
    # ä¿å­˜è§†é¢‘
    if args.save_videos:
        print("\nä¿å­˜è§†é¢‘...")
        os.makedirs("evaluate/results/quick_test", exist_ok=True)
        
        dense_path = "evaluate/results/quick_test/dense.mp4"
        sparse_path = "evaluate/results/quick_test/sparse.mp4"
        
        export_to_video(video_dense, dense_path, fps=8)
        export_to_video(video_sparse, sparse_path, fps=8)
        
        print(f"âœ“ å¯†é›†ç‰ˆ: {dense_path}")
        print(f"âœ“ ç¨€ç–ç‰ˆ: {sparse_path}")
        print("\nè¯·äººå·¥å¯¹æ¯”ä¸¤ä¸ªè§†é¢‘çš„è´¨é‡")
    
    print("\n" + "="*60)
    print("æµ‹è¯•å®Œæˆï¼")
    print("="*60)
    
    return quality


if __name__ == "__main__":
    main()

