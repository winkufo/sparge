# ç¨€ç–æ³¨æ„åŠ›å‡†ç¡®æ€§æµ‹è¯•æŒ‡å—

æœ¬ç›®å½•åŒ…å«äº†å®Œæ•´çš„æµ‹è¯•å¥—ä»¶ï¼Œç”¨äºéªŒè¯SpargeAttnç¨€ç–æ³¨æ„åŠ›çš„è¾“å‡ºå‡†ç¡®æ€§ã€‚

## ğŸ“‹ æµ‹è¯•æ–‡ä»¶æ¦‚è§ˆ

| æ–‡ä»¶ | ç”¨é€” | é€‚ç”¨åœºæ™¯ |
|------|------|----------|
| `test_sparse_attention_accuracy.py` | å®Œæ•´çš„å‡†ç¡®æ€§æµ‹è¯•å¥—ä»¶ | å…¨é¢æµ‹è¯•ï¼ŒCI/CD |
| `quick_test.py` | å¿«é€ŸéªŒè¯è„šæœ¬ | æ—¥å¸¸å¼€å‘ï¼Œå¿«é€ŸéªŒè¯ |
| `test_end_to_end_model.py` | ç«¯åˆ°ç«¯æ¨¡å‹æµ‹è¯• | é›†æˆæµ‹è¯•ï¼Œæ¨¡å‹çº§éªŒè¯ |
| `test_llama_output_accuracy.py` | **LLaMAçœŸå®æ¨¡å‹æµ‹è¯•** | **æœ€é‡è¦ï¼šæµ‹è¯•çœŸå®è¾“å‡ºå·®å¼‚** |
| `quick_llama_test.py` | LLaMAå¿«é€Ÿæµ‹è¯• | å¿«é€ŸéªŒè¯çœŸå®æ•°æ®ç¨€ç–åº¦ |

**âš ï¸ é‡è¦è¯´æ˜ï¼š**
- å‰3ä¸ªæ–‡ä»¶ä½¿ç”¨**éšæœºæ•°æ®**ï¼Œæ— æ³•åæ˜ çœŸå®ç¨€ç–åŒ–æ•ˆæœ
- **æ¨èä½¿ç”¨** `test_llama_output_accuracy.py` åœ¨çœŸå®æ¨¡å‹ä¸Šæµ‹è¯•
- çœŸå®æ¨¡å‹çš„attention patternæœ‰ç»“æ„ï¼Œç¨€ç–åº¦ä¼š**æ˜æ˜¾æ›´é«˜**

## ğŸš€ å¿«é€Ÿå¼€å§‹

### ğŸŒŸ æ¨èï¼šæµ‹è¯•çœŸå®LLaMAæ¨¡å‹

**è¿™æ˜¯æœ€å‡†ç¡®çš„æµ‹è¯•æ–¹å¼**ï¼Œå› ä¸ºå®ƒä½¿ç”¨çœŸå®æ¨¡å‹çš„attention patternï¼š

```bash
# å®Œæ•´æµ‹è¯•ï¼ˆéœ€è¦5-10åˆ†é’Ÿï¼‰
python tests/test_llama_output_accuracy.py

# å¿«é€Ÿæµ‹è¯•ï¼ˆ2åˆ†é’Ÿï¼‰
python tests/quick_llama_test.py

# å¯¹æ¯”éšæœºæ•°æ®vsçœŸå®æ•°æ®
python tests/quick_llama_test.py --compare
```

**è¾“å‡ºç¤ºä¾‹**ï¼š
```
æµ‹è¯•è¾“å‡ºå·®å¼‚
============================================================
æ±‡æ€»ç»Ÿè®¡ (æ‰€æœ‰æ ·æœ¬å¹³å‡)
============================================================
logits_cosine       : 0.987654 (Â±0.003210)
logits_l1           : 0.043210 (Â±0.012345)
token_accuracy      : 0.945000 (Â±0.023456)
top5_overlap        : 0.980000 (Â±0.015432)

æœ€ç»ˆè¯„ä¼°
============================================================
å…³é”®æŒ‡æ ‡:
  Logitsç›¸ä¼¼åº¦: 0.9877
  Tokenå‡†ç¡®ç‡: 94.50%
  å¹³å‡ç¨€ç–åº¦: 42.35%

è¯„ä¼°:
  âœ“ è‰¯å¥½ï¼å¯ä»¥ç”¨äºç”Ÿäº§
  âœ“ ç¨€ç–åº¦å¾ˆå¥½ (42.4%)
```

---

### 1. å¿«é€ŸéªŒè¯éšæœºæ•°æ®ï¼ˆä»…ç”¨äºåŠŸèƒ½æµ‹è¯•ï¼‰

âš ï¸ **æ³¨æ„**ï¼šéšæœºæ•°æ®çš„ç¨€ç–åº¦ä¼šå¾ˆä½ï¼Œä¸ä»£è¡¨çœŸå®æ•ˆæœ

```bash
cd /path/to/SpargeAttn
python tests/quick_test.py
```

**é«˜çº§ç”¨æ³•ï¼š**

```bash
# æµ‹è¯•é•¿åºåˆ—
python tests/quick_test.py --seq_len 4096

# æµ‹è¯•é«˜ç¨€ç–åº¦
python tests/quick_test.py --sparse_level high

# æµ‹è¯•causal attention
python tests/quick_test.py --causal

# è¿è¡Œé€Ÿåº¦benchmark
python tests/quick_test.py --benchmark
```

**è¾“å‡ºç¤ºä¾‹ï¼š**

```
============================================================
å¿«é€Ÿæµ‹è¯•: ä¸­ç¨€ç–åº¦-ä¸­ç²¾åº¦
åºåˆ—é•¿åº¦: 1024, Causal: False
============================================================

è¾“å…¥å½¢çŠ¶: Q=torch.Size([2, 8, 1024, 64]), ...

è®¡ç®—æ ‡å‡†attention (baseline)...
è®¡ç®—ç¨€ç–attention...

ç²¾åº¦æŒ‡æ ‡:
------------------------------------------------------------
Cossim: 0.9876, L1: 0.0543, RMSE:0.0892
ç¨€ç–åº¦: 45.23%

æµ‹è¯•ç»“æœ:
------------------------------------------------------------
âœ“ Cosineç›¸ä¼¼åº¦ 0.9876 > 0.95
âœ“ L1è¯¯å·® 0.0543 < 0.1
âœ“ ç¨€ç–åº¦ 45.23% > 5%

============================================================
âœ“ æµ‹è¯•é€šè¿‡!
============================================================
```

### 2. å®Œæ•´æµ‹è¯•å¥—ä»¶

è¿è¡Œæ‰€æœ‰æµ‹è¯•ï¼Œç”Ÿæˆè¯¦ç»†æŠ¥å‘Šï¼š

```bash
python tests/test_sparse_attention_accuracy.py
```

è¿™å°†ï¼š
- âœ… æµ‹è¯•åŸºç¡€å‡†ç¡®æ€§
- âœ… æµ‹è¯•ä¸åŒç¨€ç–åº¦çº§åˆ«
- âœ… æµ‹è¯•ä¸åŒåºåˆ—é•¿åº¦
- âœ… æµ‹è¯•causal vs non-causal
- âœ… æµ‹è¯•æ•°å€¼ç¨³å®šæ€§
- âœ… æµ‹è¯•è¾¹ç•Œæ¡ä»¶
- âœ… ç”Ÿæˆå¯è§†åŒ–å¯¹æ¯”å›¾
- âœ… ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š

**è¾“å‡ºæ–‡ä»¶ï¼š**
- `attention_comparison.png` - attention mapå¯¹æ¯”å›¾
- `accuracy_test_report.txt` - è¯¦ç»†æµ‹è¯•æŠ¥å‘Š

### 3. ä½¿ç”¨PyTestè¿è¡Œ

```bash
# å®‰è£…pytest
pip install pytest pytest-cov

# è¿è¡Œæ‰€æœ‰æµ‹è¯•
pytest tests/test_sparse_attention_accuracy.py -v

# è¿è¡Œç‰¹å®šæµ‹è¯•
pytest tests/test_sparse_attention_accuracy.py::TestSparseAttentionAccuracy::test_basic_correctness -v

# ç”Ÿæˆè¦†ç›–ç‡æŠ¥å‘Š
pytest tests/ --cov=spas_sage_attn --cov-report=html
```

### 4. ç«¯åˆ°ç«¯æ¨¡å‹æµ‹è¯•

æµ‹è¯•åœ¨å®Œæ•´æ¨¡å‹ä¸­çš„è¡¨ç°ï¼š

```bash
python tests/test_end_to_end_model.py
```

è¿™å°†æµ‹è¯•ï¼š
- å•å±‚Transformer block
- å¤šå±‚Transformeræ¨¡å‹
- ä¸åŒåºåˆ—é•¿åº¦ä¸‹çš„ç¨³å®šæ€§
- æ˜¾å­˜ä½¿ç”¨å¯¹æ¯”
- Attention patternä¿ç•™æƒ…å†µ

## ğŸ¯ æµ‹è¯•æ–¹æ³•è®º

### ä¸ºä»€ä¹ˆæµ‹è¯•è¾“å‡ºå·®å¼‚è€Œä¸æ˜¯å›°æƒ‘åº¦ï¼Ÿ

**é—®é¢˜**ï¼šä½ å¯èƒ½æƒ³é—®ï¼Œä¸ºä»€ä¹ˆä¸æµ‹è¯•å›°æƒ‘åº¦(Perplexity)ï¼Ÿ

**ç­”æ¡ˆ**ï¼šå›°æƒ‘åº¦æ˜¯è¯„ä¼°**å•ä¸ªæ¨¡å‹å¥½å**çš„æŒ‡æ ‡ï¼Œä½†æˆ‘ä»¬è¦æµ‹è¯•çš„æ˜¯**ç¨€ç–åŒ–å‰åçš„å·®å¼‚**ã€‚

```python
# âŒ é”™è¯¯çš„æµ‹è¯•æ€è·¯
ppl_dense = compute_perplexity(dense_model)    # 10.5
ppl_sparse = compute_perplexity(sparse_model)  # 10.8
# â†’ å›°æƒ‘åº¦å˜åŒ–äº†ï¼Œä½†è¿™ä¸èƒ½ç›´æ¥è¯´æ˜ç¨€ç–åŒ–çš„å½±å“

# âœ… æ­£ç¡®çš„æµ‹è¯•æ€è·¯  
# ç›¸åŒè¾“å…¥ â†’ å¯¹æ¯”è¾“å‡º
same_input = "The capital of France is"
logits_dense = dense_model(same_input)
logits_sparse = sparse_model(same_input)
difference = cosine_similarity(logits_dense, logits_sparse)
# â†’ 0.987ï¼Œè¯´æ˜è¾“å‡ºå‡ ä¹ä¸€è‡´ï¼
```

### æ ¸å¿ƒæµ‹è¯•æŒ‡æ ‡

**æˆ‘ä»¬æµ‹è¯•çš„æ˜¯ï¼šå¯†é›†æ¨¡å‹ vs ç¨€ç–æ¨¡å‹åœ¨ç›¸åŒè¾“å…¥ä¸‹çš„è¾“å‡ºå·®å¼‚**

| æŒ‡æ ‡ | å«ä¹‰ | å¥½çš„æ ‡å‡† |
|------|------|---------|
| **Logits Cosineç›¸ä¼¼åº¦** | è¾“å‡ºå‘é‡çš„æ–¹å‘ä¸€è‡´æ€§ | > 0.95 |
| **Logits L1è¯¯å·®** | è¾“å‡ºæ•°å€¼çš„ç›¸å¯¹è¯¯å·® | < 0.10 |
| **Tokenå‡†ç¡®ç‡** | é¢„æµ‹çš„tokenå®Œå…¨ç›¸åŒçš„æ¯”ä¾‹ | > 90% |
| **Top-ké‡å ç‡** | å‰kä¸ªå€™é€‰tokençš„é‡å  | > 95% |
| **ç”Ÿæˆä¸€è‡´æ€§** | ç”Ÿæˆçš„æ–‡æœ¬æ˜¯å¦ç›¸åŒ | å®šæ€§è¯„ä¼° |

### çœŸå®æ•°æ® vs éšæœºæ•°æ®

**å…³é”®å‘ç°**ï¼šä½ çš„è§‚å¯Ÿå®Œå…¨æ­£ç¡®ï¼

```python
# éšæœºæ•°æ®æµ‹è¯•
q_random = torch.randn(1, 8, 1024, 64)
k_random = torch.randn(1, 8, 1024, 64)
v_random = torch.randn(1, 8, 1024, 64)
sparsity_random = test_sparse_attn(q_random, k_random, v_random)
# â†’ ç¨€ç–åº¦: 15% (å—å†…ç›¸ä¼¼åº¦å¾ˆä½ï¼Œå‡ ä¹ä¸èƒ½ç¨€ç–)

# çœŸå®LLaMAæ•°æ®æµ‹è¯•
q_real, k_real, v_real = extract_from_llama(real_text)
sparsity_real = test_sparse_attn(q_real, k_real, v_real)
# â†’ ç¨€ç–åº¦: 45% (attention patternæœ‰ç»“æ„ï¼Œå¯ä»¥å¤§é‡ç¨€ç–ï¼)
```

**ç»“è®º**ï¼š
- âœ… å¿…é¡»åœ¨çœŸå®æ¨¡å‹ä¸Šæµ‹è¯•
- âŒ éšæœºæ•°æ®åªèƒ½æµ‹è¯•"åŠŸèƒ½æ˜¯å¦æ­£å¸¸"
- âœ… çœŸå®æ•°æ®æ‰èƒ½æµ‹è¯•"ç¨€ç–åŒ–æ•ˆæœå¦‚ä½•"

---

## ğŸ“Š ç†è§£æµ‹è¯•æŒ‡æ ‡

### 1. **Cosineç›¸ä¼¼åº¦ (Cosine Similarity)**

```python
sim = F.cosine_similarity(output_sparse.flatten(), output_baseline.flatten())
```

- **èŒƒå›´**: -1 åˆ° 1
- **å«ä¹‰**: ä¸¤ä¸ªè¾“å‡ºå‘é‡çš„æ–¹å‘ç›¸ä¼¼åº¦
- **é˜ˆå€¼å»ºè®®**:
  - `> 0.99`: æé«˜ç²¾åº¦ï¼ˆå‡ ä¹å®Œå…¨ä¸€è‡´ï¼‰
  - `> 0.95`: é«˜ç²¾åº¦ï¼ˆæ¨èï¼‰
  - `> 0.90`: ä¸­ç­‰ç²¾åº¦
  - `< 0.90`: ä½ç²¾åº¦ï¼ˆéœ€è¦è°ƒæ•´å‚æ•°ï¼‰

### 2. **L1ç›¸å¯¹è¯¯å·®**

```python
l1 = (output_sparse - output_baseline).abs().sum() / output_baseline.abs().sum()
```

- **èŒƒå›´**: 0 åˆ° âˆ
- **å«ä¹‰**: ç›¸å¯¹å¹³å‡ç»å¯¹è¯¯å·®
- **é˜ˆå€¼å»ºè®®**:
  - `< 0.05`: æé«˜ç²¾åº¦
  - `< 0.10`: é«˜ç²¾åº¦ï¼ˆæ¨èï¼‰
  - `< 0.15`: ä¸­ç­‰ç²¾åº¦
  - `> 0.15`: ä½ç²¾åº¦

### 3. **RMSE (å‡æ–¹æ ¹è¯¯å·®)**

```python
rmse = torch.sqrt(torch.mean((output_sparse - output_baseline) ** 2))
```

- **èŒƒå›´**: 0 åˆ° âˆ
- **å«ä¹‰**: è¯¯å·®çš„æ ‡å‡†å·®
- **ç‰¹ç‚¹**: å¯¹å¤§è¯¯å·®æ›´æ•æ„Ÿ

### 4. **ç¨€ç–åº¦ (Sparsity)**

```python
sparsity = 1 - (æœ‰æ•ˆå—æ•° / æ€»å—æ•°)
```

- **èŒƒå›´**: 0 åˆ° 1
- **å«ä¹‰**: è·³è¿‡çš„è®¡ç®—æ¯”ä¾‹
- **æœŸæœ›**: 
  - æ›´é«˜çš„ç¨€ç–åº¦ = æ›´å¿«çš„é€Ÿåº¦
  - éœ€è¦ä¸ç²¾åº¦æƒè¡¡

## ğŸ¯ æµ‹è¯•ç­–ç•¥å»ºè®®

### åœºæ™¯1: å¼€å‘æ–°åŠŸèƒ½

```bash
# å¿«é€ŸéªŒè¯åŠŸèƒ½æ˜¯å¦å·¥ä½œ
python tests/quick_test.py

# é€šè¿‡åå†è¿è¡Œå®Œæ•´æµ‹è¯•
python tests/test_sparse_attention_accuracy.py
```

### åœºæ™¯2: è°ƒæ•´è¶…å‚æ•°

åˆ›å»ºè‡ªå®šä¹‰æµ‹è¯•ï¼š

```python
from tests.test_sparse_attention_accuracy import SparseAttentionTester

tester = SparseAttentionTester()

# æµ‹è¯•ä½ çš„å‚æ•°ç»„åˆ
for simth in [0.3, 0.5, 0.7]:
    for cdfth in [0.95, 0.97, 0.99]:
        metrics = tester.test_basic_accuracy(
            simthreshd1=simth,
            cdfthreshd=cdfth
        )
        print(f"simth={simth}, cdfth={cdfth}: "
              f"Cosine={metrics['Cossim']:.4f}, "
              f"Sparsity={metrics['sparsity']:.2%}")
```

### åœºæ™¯3: é›†æˆåˆ°æ–°æ¨¡å‹

```python
# 1. å…ˆç”¨å¿«é€Ÿæµ‹è¯•éªŒè¯åŸºæœ¬åŠŸèƒ½
python tests/quick_test.py --seq_len 2048

# 2. åˆ›å»ºæ¨¡å‹çº§æµ‹è¯•ï¼ˆå‚è€ƒtest_end_to_end_model.pyï¼‰
# 3. å¯¹æ¯”ç”Ÿæˆç»“æœï¼ˆå¦‚å›¾åƒã€è§†é¢‘è´¨é‡ï¼‰
```

### åœºæ™¯4: CI/CDé›†æˆ

```yaml
# .github/workflows/test.yml ç¤ºä¾‹
name: Accuracy Tests

on: [push, pull_request]

jobs:
  test:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v2
    
    - name: Setup Python
      uses: actions/setup-python@v2
      with:
        python-version: 3.9
    
    - name: Install dependencies
      run: |
        pip install -r requirements.txt
        python setup.py install
    
    - name: Run quick test
      run: python tests/quick_test.py
    
    - name: Run pytest
      run: pytest tests/test_sparse_attention_accuracy.py -v
```

## ğŸ” è°ƒè¯•å¤±è´¥çš„æµ‹è¯•

### å¦‚æœCosineç›¸ä¼¼åº¦ä½

```python
# 1. æ£€æŸ¥è¾“å…¥æ•°æ®ç±»å‹
print(q.dtype, k.dtype, v.dtype)  # åº”è¯¥æ˜¯float16æˆ–bfloat16

# 2. é™ä½ç¨€ç–åº¦
metrics = tester.test_basic_accuracy(
    simthreshd1=0.3,   # é™ä½é˜ˆå€¼
    cdfthreshd=0.99     # æé«˜ä¿ç•™æ¯”ä¾‹
)

# 3. å¯è§†åŒ–å·®å¼‚
tester.visualize_attention_comparison(q, k, v)
```

### å¦‚æœL1è¯¯å·®å¤§

```python
# æ£€æŸ¥æ˜¯å¦æœ‰NaNæˆ–Inf
print(f"Output has NaN: {torch.isnan(output).any()}")
print(f"Output has Inf: {torch.isinf(output).any()}")

# æ£€æŸ¥æ•°å€¼èŒƒå›´
print(f"Output range: [{output.min():.4f}, {output.max():.4f}]")
```

### å¦‚æœç¨€ç–åº¦å¤ªä½

```python
# 1. æ£€æŸ¥åºåˆ—é•¿åº¦ï¼ˆå¤ªçŸ­å¯èƒ½ç¨€ç–ä¸èµ·æ¥ï¼‰
# æœ€å°æ¨è: 512

# 2. è°ƒæ•´å‚æ•°
simthreshd1 = 0.7   # æ›´é«˜çš„é˜ˆå€¼
cdfthreshd = 0.95   # æ›´ä½çš„CDFé˜ˆå€¼
```

## ğŸ“ˆ å¯è§†åŒ–å·¥å…·

### ç”ŸæˆAttention Mapå¯¹æ¯”

```python
from tests.test_sparse_attention_accuracy import SparseAttentionTester

tester = SparseAttentionTester()
q, k, v = tester.generate_test_data(seq_len=512)
tester.visualize_attention_comparison(q, k, v, head_idx=0, 
                                     save_path="my_comparison.png")
```

### åˆ†æç¨€ç–æ¨¡å¼

```python
from spas_sage_attn.utils import get_block_map_meansim

# è·å–å—ç¨€ç–mask
block_map = get_block_map_meansim(
    q, k,
    simthreshd1=0.6,
    cdfthreshd=0.98,
    return_lut=False
)

# å¯è§†åŒ–ç¨€ç–æ¨¡å¼
import matplotlib.pyplot as plt
plt.figure(figsize=(10, 8))
plt.imshow(block_map[0, 0].cpu().numpy(), cmap='binary')
plt.title('Block Sparse Pattern (Head 0)')
plt.xlabel('Key Blocks')
plt.ylabel('Query Blocks')
plt.colorbar()
plt.savefig('sparse_pattern.png')
```

## ğŸ“ æœ€ä½³å®è·µ

1. **å§‹ç»ˆå…ˆè¿è¡Œå¿«é€Ÿæµ‹è¯•**
   ```bash
   python tests/quick_test.py
   ```

2. **è°ƒå‚æ—¶ä½¿ç”¨ç½‘æ ¼æœç´¢**
   ```python
   for s in [0.3, 0.5, 0.7]:
       for c in [0.95, 0.97, 0.99]:
           test_basic_accuracy(simthreshd1=s, cdfthreshd=c)
   ```

3. **å…³æ³¨ç²¾åº¦-ç¨€ç–åº¦æƒè¡¡**
   - ç›®æ ‡: Cosine > 0.95 ä¸” Sparsity > 30%

4. **åœ¨çœŸå®æ•°æ®ä¸ŠéªŒè¯**
   - åˆæˆæ•°æ®æµ‹è¯•é€šè¿‡åï¼Œç”¨å®é™…æ¨¡å‹æ•°æ®æµ‹è¯•

5. **è®°å½•æµ‹è¯•ç»“æœ**
   ```python
   tester.generate_report(save_path="my_test_report.txt")
   ```

## ğŸ¤ è´¡çŒ®æµ‹è¯•ç”¨ä¾‹

æ¬¢è¿æ·»åŠ æ–°çš„æµ‹è¯•ç”¨ä¾‹ï¼è¯·ç¡®ä¿ï¼š

1. âœ… æµ‹è¯•è¦†ç›–ç‰¹å®šåœºæ™¯
2. âœ… åŒ…å«æ¸…æ™°çš„æ³¨é‡Š
3. âœ… æä¾›é¢„æœŸç»“æœ
4. âœ… å¯ä»¥ç‹¬ç«‹è¿è¡Œ

ç¤ºä¾‹PRç»“æ„ï¼š
```
tests/
  test_long_sequence.py      # æ–°æµ‹è¯•æ–‡ä»¶
  README.md                  # æ›´æ–°æ–‡æ¡£
```

## ğŸ“ å¸¸è§é—®é¢˜

**Q: æµ‹è¯•è¿è¡Œå¾ˆæ…¢æ€ä¹ˆåŠï¼Ÿ**

A: ä½¿ç”¨å¿«é€Ÿæµ‹è¯•æˆ–å‡å°æµ‹è¯•è§„æ¨¡ï¼š
```bash
python tests/quick_test.py --seq_len 512
```

**Q: å¦‚ä½•æµ‹è¯•ç‰¹å®šhead_dimï¼Ÿ**

A: ä¿®æ”¹æµ‹è¯•æ•°æ®ç”Ÿæˆå™¨ï¼š
```python
q, k, v = tester.generate_test_data(head_dim=128)
```

**Q: èƒ½æµ‹è¯•FP32è¾“å…¥å—ï¼Ÿ**

A: ç¨€ç–kernelä¼šè‡ªåŠ¨è½¬æ¢ä¸ºFP16/BF16ï¼Œå»ºè®®ç›´æ¥ç”¨FP16æµ‹è¯•ã€‚

**Q: å¦‚ä½•æµ‹è¯•è‡ªå·±çš„æ•°æ®ï¼Ÿ**

A:
```python
# åŠ è½½ä½ çš„æ•°æ®
my_q = torch.load('my_q.pt')
my_k = torch.load('my_k.pt')
my_v = torch.load('my_v.pt')

# è¿è¡Œæµ‹è¯•
o_baseline = tester.compute_baseline(my_q, my_k, my_v)
o_sparse = spas_sage2_attn_meansim_cuda(my_q, my_k, my_v)
metrics = precision_metric(o_sparse, o_baseline)
```

## ğŸ“§ è”ç³»ä¸æ”¯æŒ

å¦‚æœ‰é—®é¢˜ï¼Œè¯·ï¼š
1. æŸ¥çœ‹é¡¹ç›®ä¸»README
2. æäº¤GitHub Issue
3. å‚è€ƒè®ºæ–‡: [SpargeAttn Paper](https://arxiv.org/abs/2502.18137)

