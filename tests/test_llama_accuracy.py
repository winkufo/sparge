"""
åœ¨çœŸå®LLaMA 3æ¨¡å‹ä¸Šæµ‹è¯•SpargeAttnçš„å‡†ç¡®æ€§

æµ‹è¯•ç­–ç•¥ï¼š
1. åŠ è½½é¢„è®­ç»ƒLLaMA 3æ¨¡å‹
2. æ›¿æ¢attentionå±‚ä¸ºç¨€ç–ç‰ˆæœ¬
3. æµ‹è¯•perplexity (å›°æƒ‘åº¦) - è¯­è¨€æ¨¡å‹æœ€å…³é”®æŒ‡æ ‡
4. æµ‹è¯•ç”Ÿæˆè´¨é‡ - äººç±»å¯è¯»æ€§è¯„ä¼°
5. æµ‹è¯•å„ç§NLPä»»åŠ¡çš„å‡†ç¡®åº¦
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from transformers import AutoModelForCausalLM, AutoTokenizer
from datasets import load_dataset
import numpy as np
from tqdm import tqdm
import json
from typing import Dict, List, Optional
import argparse

from spas_sage_attn import spas_sage2_attn_meansim_cuda
from spas_sage_attn.utils import precision_metric


class SparseAttentionWrapper(nn.Module):
    """
    åŒ…è£…å™¨ï¼šæ›¿æ¢æ ‡å‡†attentionä¸ºç¨€ç–attention
    ä¿æŒæ¥å£å…¼å®¹LLaMAçš„Attentionæ¨¡å—
    """
    
    def __init__(self, original_attn, simthreshd1=0.6, cdfthreshd=0.98):
        super().__init__()
        self.original_attn = original_attn
        self.simthreshd1 = simthreshd1
        self.cdfthreshd = cdfthreshd
        self.use_sparse = True  # å¯ä»¥åŠ¨æ€åˆ‡æ¢
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.total_calls = 0
        self.total_sparsity = 0.0
    
    def forward(
        self,
        hidden_states: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.Tensor] = None,
        past_key_value=None,
        output_attentions: bool = False,
        use_cache: bool = False,
        **kwargs,
    ):
        """
        ä¸LLaMA Attentionæ¥å£å…¼å®¹çš„forwardå‡½æ•°
        """
        if not self.use_sparse:
            # ä½¿ç”¨åŸå§‹attention
            return self.original_attn(
                hidden_states,
                attention_mask=attention_mask,
                position_ids=position_ids,
                past_key_value=past_key_value,
                output_attentions=output_attentions,
                use_cache=use_cache,
                **kwargs
            )
        
        # è·å–åŸå§‹attentionçš„Q, K, VæŠ•å½±
        bsz, q_len, _ = hidden_states.size()
        
        # ä½¿ç”¨åŸå§‹æ¨¡å‹çš„projection
        query_states = self.original_attn.q_proj(hidden_states)
        key_states = self.original_attn.k_proj(hidden_states)
        value_states = self.original_attn.v_proj(hidden_states)
        
        # Reshape to multi-head format
        query_states = query_states.view(bsz, q_len, self.original_attn.num_heads, self.original_attn.head_dim).transpose(1, 2)
        key_states = key_states.view(bsz, q_len, self.original_attn.num_key_value_heads, self.original_attn.head_dim).transpose(1, 2)
        value_states = value_states.view(bsz, q_len, self.original_attn.num_key_value_heads, self.original_attn.head_dim).transpose(1, 2)
        
        # Handle KV cache
        if past_key_value is not None:
            key_states = torch.cat([past_key_value[0], key_states], dim=2)
            value_states = torch.cat([past_key_value[1], value_states], dim=2)
        
        # GQA: repeat k/v heads if necessary
        if self.original_attn.num_key_value_heads != self.original_attn.num_heads:
            key_states = self._repeat_kv(key_states, self.original_attn.num_key_value_groups)
            value_states = self._repeat_kv(value_states, self.original_attn.num_key_value_groups)
        
        # ä½¿ç”¨ç¨€ç–attention
        kv_seq_len = key_states.shape[-2]
        
        # åªåœ¨åºåˆ—è¶³å¤Ÿé•¿æ—¶ä½¿ç”¨ç¨€ç–
        if kv_seq_len >= 512:
            try:
                attn_output, sparsity = spas_sage2_attn_meansim_cuda(
                    query_states,
                    key_states,
                    value_states,
                    is_causal=True,  # LLaMAä½¿ç”¨causal mask
                    simthreshd1=self.simthreshd1,
                    cdfthreshd=self.cdfthreshd,
                    return_sparsity=True
                )
                
                # ç»Ÿè®¡ç¨€ç–åº¦
                self.total_calls += 1
                self.total_sparsity += sparsity
                
            except Exception as e:
                # Fallbackåˆ°æ ‡å‡†attention
                print(f"Sparse attention failed: {e}, using dense attention")
                attn_output = F.scaled_dot_product_attention(
                    query_states, key_states, value_states, is_causal=True
                )
        else:
            # çŸ­åºåˆ—ä½¿ç”¨æ ‡å‡†attention
            attn_output = F.scaled_dot_product_attention(
                query_states, key_states, value_states, is_causal=True
            )
        
        # Reshape back
        attn_output = attn_output.transpose(1, 2).contiguous()
        attn_output = attn_output.reshape(bsz, q_len, self.original_attn.hidden_size)
        
        # Output projection
        attn_output = self.original_attn.o_proj(attn_output)
        
        if use_cache:
            return attn_output, None, (key_states, value_states)
        else:
            return attn_output, None, None
    
    def _repeat_kv(self, hidden_states, n_rep):
        """GQA: repeat k/v heads"""
        batch, num_key_value_heads, slen, head_dim = hidden_states.shape
        if n_rep == 1:
            return hidden_states
        hidden_states = hidden_states[:, :, None, :, :].expand(
            batch, num_key_value_heads, n_rep, slen, head_dim
        )
        return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
    
    def get_avg_sparsity(self):
        """è·å–å¹³å‡ç¨€ç–åº¦"""
        if self.total_calls == 0:
            return 0.0
        return self.total_sparsity / self.total_calls


def replace_attention_with_sparse(model, simthreshd1=0.6, cdfthreshd=0.98):
    """
    æ›¿æ¢æ¨¡å‹ä¸­æ‰€æœ‰çš„attentionå±‚ä¸ºç¨€ç–ç‰ˆæœ¬
    """
    replaced_count = 0
    
    for name, module in model.named_modules():
        # LLaMA 3çš„attentionæ¨¡å—é€šå¸¸å‘½åä¸º 'self_attn'
        if hasattr(module, 'self_attn'):
            original_attn = module.self_attn
            module.self_attn = SparseAttentionWrapper(
                original_attn,
                simthreshd1=simthreshd1,
                cdfthreshd=cdfthreshd
            )
            replaced_count += 1
            print(f"æ›¿æ¢äº† {name}.self_attn")
    
    print(f"\næ€»å…±æ›¿æ¢äº† {replaced_count} ä¸ªattentionå±‚")
    return model


def compute_perplexity(
    model,
    tokenizer,
    dataset_name="wikitext",
    dataset_config="wikitext-2-raw-v1",
    max_samples=100,
    max_length=2048
):
    """
    è®¡ç®—æ¨¡å‹çš„å›°æƒ‘åº¦ï¼ˆPerplexityï¼‰
    
    è¿™æ˜¯è¯„ä¼°è¯­è¨€æ¨¡å‹æœ€é‡è¦çš„æŒ‡æ ‡ï¼š
    - PPLè¶Šä½è¶Šå¥½
    - PPL = exp(average negative log-likelihood)
    """
    print(f"\n{'='*60}")
    print(f"è®¡ç®—Perplexity on {dataset_name}")
    print(f"{'='*60}")
    
    # åŠ è½½æ•°æ®é›†
    if dataset_name == "wikitext":
        dataset = load_dataset(dataset_name, dataset_config, split="test")
        texts = [item['text'] for item in dataset if len(item['text']) > 100]
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æ•°æ®é›†: {dataset_name}")
    
    # é™åˆ¶æ ·æœ¬æ•°
    texts = texts[:max_samples]
    
    model.eval()
    total_loss = 0.0
    total_tokens = 0
    
    with torch.no_grad():
        for text in tqdm(texts, desc="è®¡ç®—å›°æƒ‘åº¦"):
            # Tokenize
            encodings = tokenizer(
                text,
                return_tensors="pt",
                max_length=max_length,
                truncation=True
            ).to(model.device)
            
            input_ids = encodings.input_ids
            
            if input_ids.size(1) < 2:
                continue
            
            # Forward pass
            outputs = model(input_ids, labels=input_ids)
            loss = outputs.loss
            
            # ç´¯ç§¯
            total_loss += loss.item() * input_ids.size(1)
            total_tokens += input_ids.size(1)
    
    # è®¡ç®—å¹³å‡losså’Œperplexity
    avg_loss = total_loss / total_tokens
    perplexity = np.exp(avg_loss)
    
    print(f"Average Loss: {avg_loss:.4f}")
    print(f"Perplexity: {perplexity:.4f}")
    
    return {
        'loss': avg_loss,
        'perplexity': perplexity,
        'num_samples': len(texts),
        'num_tokens': total_tokens
    }


def test_generation_quality(
    model,
    tokenizer,
    prompts: List[str],
    max_new_tokens=100,
    temperature=0.7
):
    """
    æµ‹è¯•ç”Ÿæˆè´¨é‡
    
    é€šè¿‡å®é™…ç”Ÿæˆæ–‡æœ¬æ¥è¯„ä¼°æ¨¡å‹æ˜¯å¦è¿˜èƒ½ç”Ÿæˆåˆç†çš„å†…å®¹
    """
    print(f"\n{'='*60}")
    print("æµ‹è¯•ç”Ÿæˆè´¨é‡")
    print(f"{'='*60}")
    
    model.eval()
    generations = []
    
    with torch.no_grad():
        for prompt in prompts:
            print(f"\nPrompt: {prompt}")
            print("-" * 60)
            
            inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
            
            outputs = model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                temperature=temperature,
                do_sample=True,
                top_p=0.9
            )
            
            generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
            print(f"Generated: {generated_text}")
            
            generations.append({
                'prompt': prompt,
                'generated': generated_text
            })
    
    return generations


def test_output_consistency(
    model_dense,
    model_sparse,
    tokenizer,
    test_texts: List[str],
    max_length=1024
):
    """
    å¯¹æ¯”å¯†é›†æ¨¡å‹å’Œç¨€ç–æ¨¡å‹çš„è¾“å‡ºä¸€è‡´æ€§
    
    è¿™æ˜¯æœ€ç›´æ¥çš„æµ‹è¯•ï¼šç›¸åŒè¾“å…¥ï¼Œè¾“å‡ºåº”è¯¥æ¥è¿‘
    """
    print(f"\n{'='*60}")
    print("æµ‹è¯•è¾“å‡ºä¸€è‡´æ€§ï¼ˆæœ€å…³é”®æµ‹è¯•ï¼‰")
    print(f"{'='*60}")
    
    model_dense.eval()
    model_sparse.eval()
    
    all_metrics = []
    
    with torch.no_grad():
        for text in tqdm(test_texts, desc="å¯¹æ¯”è¾“å‡º"):
            # Tokenize
            inputs = tokenizer(
                text,
                return_tensors="pt",
                max_length=max_length,
                truncation=True
            ).to(model_dense.device)
            
            if inputs.input_ids.size(1) < 128:
                continue  # è·³è¿‡å¤ªçŸ­çš„æ–‡æœ¬
            
            # Dense model
            outputs_dense = model_dense(**inputs, output_hidden_states=True)
            logits_dense = outputs_dense.logits
            hidden_dense = outputs_dense.hidden_states[-1]
            
            # Sparse model
            outputs_sparse = model_sparse(**inputs, output_hidden_states=True)
            logits_sparse = outputs_sparse.logits
            hidden_sparse = outputs_sparse.hidden_states[-1]
            
            # å¯¹æ¯”logits
            logits_metrics = precision_metric(
                logits_sparse,
                logits_dense,
                verbose=False
            )
            
            # å¯¹æ¯”hidden states
            hidden_metrics = precision_metric(
                hidden_sparse,
                hidden_dense,
                verbose=False
            )
            
            # Token-level accuracy
            pred_dense = logits_dense.argmax(dim=-1)
            pred_sparse = logits_sparse.argmax(dim=-1)
            token_accuracy = (pred_dense == pred_sparse).float().mean().item()
            
            metrics = {
                'logits_cosine': logits_metrics['Cossim'],
                'logits_l1': logits_metrics['L1'],
                'hidden_cosine': hidden_metrics['Cossim'],
                'hidden_l1': hidden_metrics['L1'],
                'token_accuracy': token_accuracy,
                'seq_len': inputs.input_ids.size(1)
            }
            
            all_metrics.append(metrics)
    
    # æ±‡æ€»ç»Ÿè®¡
    avg_metrics = {
        key: np.mean([m[key] for m in all_metrics])
        for key in all_metrics[0].keys()
    }
    
    print(f"\nå¹³å‡æŒ‡æ ‡:")
    print(f"{'æŒ‡æ ‡':<25} {'å€¼':<10}")
    print("-" * 35)
    print(f"{'Logits Cosineç›¸ä¼¼åº¦':<25} {avg_metrics['logits_cosine']:.6f}")
    print(f"{'Logits L1è¯¯å·®':<25} {avg_metrics['logits_l1']:.6f}")
    print(f"{'Hidden Cosineç›¸ä¼¼åº¦':<25} {avg_metrics['hidden_cosine']:.6f}")
    print(f"{'Hidden L1è¯¯å·®':<25} {avg_metrics['hidden_l1']:.6f}")
    print(f"{'Tokenå‡†ç¡®ç‡':<25} {avg_metrics['token_accuracy']:.2%}")
    print(f"{'å¹³å‡åºåˆ—é•¿åº¦':<25} {avg_metrics['seq_len']:.0f}")
    
    return avg_metrics, all_metrics


def collect_sparsity_statistics(model):
    """
    æ”¶é›†æ‰€æœ‰ç¨€ç–attentionå±‚çš„ç¨€ç–åº¦ç»Ÿè®¡
    """
    print(f"\n{'='*60}")
    print("ç¨€ç–åº¦ç»Ÿè®¡")
    print(f"{'='*60}")
    
    sparsities = []
    
    for name, module in model.named_modules():
        if isinstance(module, SparseAttentionWrapper):
            avg_sparsity = module.get_avg_sparsity()
            if module.total_calls > 0:
                print(f"{name}: å¹³å‡ç¨€ç–åº¦ {avg_sparsity:.2%} "
                      f"(è°ƒç”¨æ¬¡æ•°: {module.total_calls})")
                sparsities.append(avg_sparsity)
    
    if sparsities:
        overall_avg = np.mean(sparsities)
        print(f"\næ•´ä½“å¹³å‡ç¨€ç–åº¦: {overall_avg:.2%}")
        return overall_avg
    else:
        print("æœªæ”¶é›†åˆ°ç¨€ç–åº¦æ•°æ®")
        return 0.0


def run_comprehensive_llama_test(
    model_name="meta-llama/Llama-3.2-1B",
    simthreshd1=0.6,
    cdfthreshd=0.98,
    max_samples=50,
    output_file="llama_test_results.json"
):
    """
    è¿è¡Œå®Œæ•´çš„LLaMAæµ‹è¯•å¥—ä»¶
    """
    print(f"\n{'='*80}")
    print(f"å¼€å§‹LLaMA 3æ¨¡å‹ç¨€ç–åŒ–å‡†ç¡®æ€§æµ‹è¯•")
    print(f"æ¨¡å‹: {model_name}")
    print(f"ç¨€ç–å‚æ•°: simthreshd1={simthreshd1}, cdfthreshd={cdfthreshd}")
    print(f"{'='*80}")
    
    # 1. åŠ è½½æ¨¡å‹å’Œtokenizer
    print("\n[1/7] åŠ è½½æ¨¡å‹...")
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    tokenizer.pad_token = tokenizer.eos_token
    
    # åŠ è½½åŸå§‹æ¨¡å‹ï¼ˆç”¨äºå¯¹æ¯”ï¼‰
    print("åŠ è½½å¯†é›†æ¨¡å‹ï¼ˆground truthï¼‰...")
    model_dense = AutoModelForCausalLM.from_pretrained(
        model_name,
        torch_dtype=torch.float16,
        device_map="auto"
    )
    
    # åŠ è½½ç¨€ç–æ¨¡å‹
    print("åŠ è½½ç¨€ç–æ¨¡å‹...")
    model_sparse = AutoModelForCausalLM.from_pretrained(
        model_name,
        torch_dtype=torch.float16,
        device_map="auto"
    )
    model_sparse = replace_attention_with_sparse(
        model_sparse,
        simthreshd1=simthreshd1,
        cdfthreshd=cdfthreshd
    )
    
    results = {}
    
    # 2. æµ‹è¯•Perplexityï¼ˆå¯†é›†æ¨¡å‹ï¼‰
    print("\n[2/7] è®¡ç®—å¯†é›†æ¨¡å‹çš„Perplexity...")
    ppl_dense = compute_perplexity(
        model_dense,
        tokenizer,
        max_samples=max_samples
    )
    results['perplexity_dense'] = ppl_dense
    
    # 3. æµ‹è¯•Perplexityï¼ˆç¨€ç–æ¨¡å‹ï¼‰
    print("\n[3/7] è®¡ç®—ç¨€ç–æ¨¡å‹çš„Perplexity...")
    ppl_sparse = compute_perplexity(
        model_sparse,
        tokenizer,
        max_samples=max_samples
    )
    results['perplexity_sparse'] = ppl_sparse
    
    # 4. å¯¹æ¯”å›°æƒ‘åº¦
    ppl_increase = ppl_sparse['perplexity'] - ppl_dense['perplexity']
    ppl_increase_pct = (ppl_increase / ppl_dense['perplexity']) * 100
    
    print(f"\n{'='*60}")
    print("Perplexityå¯¹æ¯”")
    print(f"{'='*60}")
    print(f"å¯†é›†æ¨¡å‹: {ppl_dense['perplexity']:.4f}")
    print(f"ç¨€ç–æ¨¡å‹: {ppl_sparse['perplexity']:.4f}")
    print(f"å¢åŠ : {ppl_increase:+.4f} ({ppl_increase_pct:+.2f}%)")
    
    if abs(ppl_increase_pct) < 2:
        print("âœ“ Perplexityå˜åŒ– < 2%, ä¼˜ç§€ï¼")
    elif abs(ppl_increase_pct) < 5:
        print("âœ“ Perplexityå˜åŒ– < 5%, è‰¯å¥½")
    elif abs(ppl_increase_pct) < 10:
        print("âš  Perplexityå˜åŒ– < 10%, å¯æ¥å—")
    else:
        print("âœ— Perplexityå˜åŒ– > 10%, éœ€è¦è°ƒæ•´å‚æ•°")
    
    results['perplexity_comparison'] = {
        'increase': ppl_increase,
        'increase_pct': ppl_increase_pct
    }
    
    # 5. è¾“å‡ºä¸€è‡´æ€§æµ‹è¯•
    print("\n[4/7] æµ‹è¯•è¾“å‡ºä¸€è‡´æ€§...")
    test_dataset = load_dataset("wikitext", "wikitext-2-raw-v1", split="test")
    test_texts = [item['text'] for item in test_dataset if len(item['text']) > 200][:20]
    
    consistency_metrics, _ = test_output_consistency(
        model_dense,
        model_sparse,
        tokenizer,
        test_texts
    )
    results['consistency'] = consistency_metrics
    
    # 6. ç”Ÿæˆè´¨é‡æµ‹è¯•
    print("\n[5/7] æµ‹è¯•ç”Ÿæˆè´¨é‡...")
    test_prompts = [
        "The capital of France is",
        "In machine learning, the most important concept is",
        "Once upon a time, there was a",
    ]
    
    print("\nå¯†é›†æ¨¡å‹ç”Ÿæˆ:")
    gen_dense = test_generation_quality(model_dense, tokenizer, test_prompts)
    
    print("\nç¨€ç–æ¨¡å‹ç”Ÿæˆ:")
    gen_sparse = test_generation_quality(model_sparse, tokenizer, test_prompts)
    
    results['generations'] = {
        'dense': gen_dense,
        'sparse': gen_sparse
    }
    
    # 7. ç¨€ç–åº¦ç»Ÿè®¡
    print("\n[6/7] æ”¶é›†ç¨€ç–åº¦ç»Ÿè®¡...")
    avg_sparsity = collect_sparsity_statistics(model_sparse)
    results['sparsity'] = avg_sparsity
    
    # 8. ç”ŸæˆæŠ¥å‘Š
    print("\n[7/7] ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š...")
    results['config'] = {
        'model_name': model_name,
        'simthreshd1': simthreshd1,
        'cdfthreshd': cdfthreshd,
        'max_samples': max_samples
    }
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    print(f"\nå®Œæ•´ç»“æœå·²ä¿å­˜è‡³: {output_file}")
    
    # æœ€ç»ˆè¯„åˆ†
    print(f"\n{'='*60}")
    print("æœ€ç»ˆè¯„ä¼°")
    print(f"{'='*60}")
    
    score = 0
    max_score = 0
    
    # Perplexityè¯„åˆ† (40åˆ†)
    max_score += 40
    if abs(ppl_increase_pct) < 2:
        score += 40
        print("âœ“ Perplexity: 40/40")
    elif abs(ppl_increase_pct) < 5:
        score += 30
        print("âœ“ Perplexity: 30/40")
    elif abs(ppl_increase_pct) < 10:
        score += 20
        print("âš  Perplexity: 20/40")
    else:
        score += 10
        print("âœ— Perplexity: 10/40")
    
    # è¾“å‡ºä¸€è‡´æ€§è¯„åˆ† (40åˆ†)
    max_score += 40
    if consistency_metrics['logits_cosine'] > 0.98:
        score += 40
        print("âœ“ è¾“å‡ºä¸€è‡´æ€§: 40/40")
    elif consistency_metrics['logits_cosine'] > 0.95:
        score += 30
        print("âœ“ è¾“å‡ºä¸€è‡´æ€§: 30/40")
    elif consistency_metrics['logits_cosine'] > 0.90:
        score += 20
        print("âš  è¾“å‡ºä¸€è‡´æ€§: 20/40")
    else:
        score += 10
        print("âœ— è¾“å‡ºä¸€è‡´æ€§: 10/40")
    
    # ç¨€ç–åº¦è¯„åˆ† (20åˆ†)
    max_score += 20
    if avg_sparsity > 0.4:
        score += 20
        print(f"âœ“ ç¨€ç–åº¦: 20/20 ({avg_sparsity:.1%})")
    elif avg_sparsity > 0.3:
        score += 15
        print(f"âœ“ ç¨€ç–åº¦: 15/20 ({avg_sparsity:.1%})")
    elif avg_sparsity > 0.2:
        score += 10
        print(f"âš  ç¨€ç–åº¦: 10/20 ({avg_sparsity:.1%})")
    else:
        score += 5
        print(f"âœ— ç¨€ç–åº¦: 5/20 ({avg_sparsity:.1%})")
    
    print(f"\næ€»åˆ†: {score}/{max_score}")
    
    if score >= 85:
        print("ğŸ‰ ä¼˜ç§€ï¼å¯ä»¥éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒ")
    elif score >= 70:
        print("âœ“ è‰¯å¥½ï¼Œé€‚åˆå¤§å¤šæ•°åº”ç”¨")
    elif score >= 50:
        print("âš  å¯æ¥å—ï¼Œå»ºè®®è°ƒæ•´å‚æ•°")
    else:
        print("âœ— éœ€è¦æ”¹è¿›")
    
    return results


def main():
    parser = argparse.ArgumentParser(description='æµ‹è¯•LLaMAæ¨¡å‹çš„ç¨€ç–åŒ–å‡†ç¡®æ€§')
    parser.add_argument('--model', type=str, default='meta-llama/Llama-3.2-1B',
                       help='æ¨¡å‹åç§°æˆ–è·¯å¾„')
    parser.add_argument('--simthreshd1', type=float, default=0.6,
                       help='ç›¸ä¼¼åº¦é˜ˆå€¼')
    parser.add_argument('--cdfthreshd', type=float, default=0.98,
                       help='CDFé˜ˆå€¼')
    parser.add_argument('--max_samples', type=int, default=50,
                       help='æµ‹è¯•æ ·æœ¬æ•°')
    parser.add_argument('--output', type=str, default='llama_test_results.json',
                       help='è¾“å‡ºæ–‡ä»¶è·¯å¾„')
    
    args = parser.parse_args()
    
    results = run_comprehensive_llama_test(
        model_name=args.model,
        simthreshd1=args.simthreshd1,
        cdfthreshd=args.cdfthreshd,
        max_samples=args.max_samples,
        output_file=args.output
    )
    
    print("\næµ‹è¯•å®Œæˆï¼")


if __name__ == "__main__":
    main()

